{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c86510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianTokenizer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5488327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files={\"train\": \"train.json\", \"validation\": \"val.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12799d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0\n",
      "True\n",
      "1\n",
      "AMD Radeon Graphics\n",
      "6.4.43482-0f2d60242\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(torch.cuda.is_available())  # Should be True if ROCm is set up\n",
    "print(torch.cuda.device_count())  # Should show number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should print your AMD GPU\n",
    "print(torch.version.hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febfe552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"rajbhirud/eng-to-fra-model\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"en\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    targets = tokenizer(batch[\"fr\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0f0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='1524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1524/1524 23:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.347800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1524, training_loss=0.5099525313990636, metrics={'train_runtime': 1401.2412, 'train_samples_per_second': 17.402, 'train_steps_per_second': 1.088, 'total_flos': 3306310921617408.0, 'train_loss': 0.5099525313990636, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MarianMTModel.from_pretrained(model_checkpoint)\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eng-fra-finetuned_0.0001_2e-5\",         # Directory where model checkpoints and final model will be saved.\n",
    "                                               # Change this if you want to save outputs elsewhere or run multiple experiments.\n",
    "\n",
    "    learning_rate=2e-5,                        # The initial learning rate for the AdamW optimizer.\n",
    "                                               # Lower values (e.g., 1e-5) can lead to slower but potentially more stable training.\n",
    "                                               # Higher values (e.g., 1e-4) may speed up training but risk overshooting minima.\n",
    "\n",
    "    per_device_train_batch_size=16,            # Number of samples per batch on each device (GPU/CPU) during training.\n",
    "                                               # Increase for faster training if you have enough memory, decrease if you get OOM errors.\n",
    "\n",
    "    per_device_eval_batch_size=32,             # Number of samples per batch on each device during evaluation.\n",
    "                                               # Can usually be set higher than training batch size if memory allows.\n",
    "\n",
    "    weight_decay=0.0001,                       # L2 regularization coefficient; helps prevent overfitting. The larger the weights, the more the model tends to overfit to the training data, not generalize\n",
    "                                               # Setting weight_decay=0.01 means 1% of each weightâ€™s value is added to the gradient during backpropagation (before the learning rate is applied).\n",
    "                                               # Increase to regularize more, decrease to regularize less.\n",
    "                                               # Higher weight_decay: Stronger regularization, less risk of overfitting, but if set too high, the model may underfit (fail to learn enough from the data). \n",
    "                                               # Lower weight_decay (or zero): Weaker or no regularization, which can lead to overfitting, especially on small datasets.\n",
    "\n",
    "    save_total_limit=1,                        # Maximum number of checkpoints to keep.\n",
    "                                               # Older checkpoints are deleted. Increase to keep more history, decrease to save disk space.\n",
    "\n",
    "    num_train_epochs=3,                        # Number of times to iterate over the entire training dataset.\n",
    "                                               # Increase for more training (may improve results with enough data), decrease for faster runs.\n",
    "\n",
    "    predict_with_generate=True,                # Use model.generate() for evaluation and prediction.\n",
    "                                               # Should be True for seq2seq tasks (like translation) to generate output sequences.\n",
    "                                               # Set to False if you only care about loss, not generated text.\n",
    "    dataloader_num_workers=4,\n",
    "    eval_steps=500\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fe866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/94 01:34 < 27:41, 0.05 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:197\u001b[39m, in \u001b[36mSeq2SeqTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m._gen_kwargs = gen_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/trainer.py:4154\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4151\u001b[39m start_time = time.time()\n\u001b[32m   4153\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4154\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4164\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/trainer.py:4348\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4345\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4347\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4348\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4349\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4350\u001b[39m inputs_decode = (\n\u001b[32m   4351\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4352\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:333\u001b[39m, in \u001b[36mSeq2SeqTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m summon_full_params_context = (\n\u001b[32m    327\u001b[39m     FullyShardedDataParallel.summon_full_params(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, FullyShardedDataParallel)\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    330\u001b[39m )\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     generated_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generation_config._from_model_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2484\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n\u001b[32m   2483\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2484\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2494\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2495\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2496\u001b[39m         batch_size=batch_size,\n\u001b[32m   2497\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2503\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2504\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3895\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3892\u001b[39m beam_indices = running_beam_indices.detach().clone()\n\u001b[32m   3894\u001b[39m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3895\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3896\u001b[39m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[32m   3897\u001b[39m     flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3898\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2602\u001b[39m, in \u001b[36mGenerationMixin._has_unfinished_sequences\u001b[39m\u001b[34m(self, this_peer_finished, synced_gpus, device)\u001b[39m\n\u001b[32m   2599\u001b[39m         result.past_key_values = result.past_key_values.to_legacy_cache()\n\u001b[32m   2600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2602\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2603\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2604\u001b[39m \u001b[33;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[32m   2605\u001b[39m \u001b[33;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[32m   2606\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[32m   2608\u001b[39m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[32m   2609\u001b[39m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# results = trainer.evaluate()\n",
    "# print(f\"BLEU score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "input_text = \"As part of Canadaâ€™s second Action Plan on Open Government, the Government of Canada has committed to the disclosure of contracting data via a centralized, machine-readable database available to the public. This commitment reinforces the proactive publication of contracts, which reflects broader government commitments to transparency and strengthened accountability within the public sector originally announced in Budget 2004.\"\n",
    "reference_text = \"Dans le cadre du deuxiÃ¨me Plan dâ€™action national pour un gouvernement ouvert du Canada, le gouvernement du Canada sâ€™est engagÃ© Ã  la divulgation des donnÃ©es sur la passation de marchÃ©s au moyen dâ€™une base de donnÃ©es publique Ã  la fois centralisÃ©e et lisible par machine. Cet engagement renforce la publication proactive des marchÃ©s, qui tient compte des engagements Ã©largis du gouvernement envers la transparence et la responsabilisation accrue dans le secteur public, annoncÃ©s initialement dans le budget fÃ©dÃ©ral de 2004.\"\n",
    "original_prediction = \"Dans le cadre du deuxiÃ¨me Plan d'action du Canada pour un gouvernement ouvert, le gouvernement du Canada s'est engagÃ© Ã  communiquer des donnÃ©es sous-traitÃ©es via une base de donnÃ©es centralisÃ©e, lisible par la machine, accessible au public. Cet objectif renforce la publication proactive des marchÃ©s, qui reflÃ¨te l'ensemble des promesses gouvernementales en matiÃ¨re de transparence et de redevabilitÃ© au sein du secteur public initialement annoncÃ© dans le budget de 2004.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}  # Move inputs to the same device as the model\n",
    "translated = model.generate(**inputs)\n",
    "print(\"Input: \", input_text)\n",
    "#print(textwrap.fill(input_text, width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Prediction (Post fine tuning): \", tokenizer.decode(translated[0], skip_special_tokens=True))\n",
    "#print(textwrap.fill(tokenizer.decode(translated[0], skip_special_tokens=True), width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Reference: \", reference_text)\n",
    "#print(textwrap.fill(reference_text, width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Original Prediction (Pre fine tuning): \", original_prediction)\n",
    "#print(textwrap.fill(original_prediction, width=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
