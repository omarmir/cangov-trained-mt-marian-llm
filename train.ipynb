{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c86510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianTokenizer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5488327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'The', 'fr': 'Les'}\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files={\"train\": \"output/all_pairs.json\", \"validation\": \"val.json\"})\n",
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12799d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febfe552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"rajbhirud/eng-to-fra-model\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"en\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    targets = tokenizer(batch[\"fr\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d143e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "AMD Radeon Graphics\n",
      "6.4.43482-0f2d60242\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should be True if ROCm is set up\n",
    "print(torch.cuda.device_count())  # Should show number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should print your AMD GPU\n",
    "print(torch.version.hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0f0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/Code/AI-Translation/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=4.011698105755975, metrics={'train_runtime': 44.7085, 'train_samples_per_second': 36.101, 'train_steps_per_second': 1.141, 'total_flos': 109423921987584.0, 'train_loss': 4.011698105755975, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MarianMTModel.from_pretrained(model_checkpoint)\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eng-fra-finetuned\",         # Directory where model checkpoints and final model will be saved.\n",
    "                                               # Change this if you want to save outputs elsewhere or run multiple experiments.\n",
    "\n",
    "    learning_rate=1e-5,                        # The initial learning rate for the AdamW optimizer.\n",
    "                                               # Lower values (e.g., 1e-5) can lead to slower but potentially more stable training.\n",
    "                                               # Higher values (e.g., 1e-4) may speed up training but risk overshooting minima.\n",
    "\n",
    "    per_device_train_batch_size=32,            # Number of samples per batch on each device (GPU/CPU) during training.\n",
    "                                               # Increase for faster training if you have enough memory, decrease if you get OOM errors.\n",
    "\n",
    "    per_device_eval_batch_size=64,             # Number of samples per batch on each device during evaluation.\n",
    "                                               # Can usually be set higher than training batch size if memory allows.\n",
    "\n",
    "    weight_decay=0.001,                        # L2 regularization coefficient; helps prevent overfitting.\n",
    "                                               # Increase to regularize more, decrease to regularize less.\n",
    "                                               # Higher weight_decay: Stronger regularization, less risk of overfitting, but if set too high, the model may underfit (fail to learn enough from the data). \n",
    "                                               # Lower weight_decay (or zero): Weaker or no regularization, which can lead to overfitting, especially on small datasets.\n",
    "\n",
    "    save_total_limit=3,                        # Maximum number of checkpoints to keep.\n",
    "                                               # Older checkpoints are deleted. Increase to keep more history, decrease to save disk space.\n",
    "\n",
    "    num_train_epochs=3,                       # Number of times to iterate over the entire training dataset.\n",
    "                                               # Increase for more training (may improve results with enough data), decrease for faster runs.\n",
    "\n",
    "    predict_with_generate=True,                # Use model.generate() for evaluation and prediction.\n",
    "                                               # Should be True for seq2seq tasks (like translation) to generate output sequences.\n",
    "                                               # Set to False if you only care about loss, not generated text.\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58fe866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: {'eval_loss': 0.6805958151817322, 'eval_score': 0.0, 'eval_counts': [0, 0, 0, 0], 'eval_totals': [0, 0, 0, 0], 'eval_precisions': [0.0, 0.0, 0.0, 0.0], 'eval_bp': 0.0, 'eval_sys_len': 0, 'eval_ref_len': 4, 'eval_runtime': 0.3268, 'eval_samples_per_second': 3.06, 'eval_steps_per_second': 3.06, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"BLEU score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56275cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  As part of Canada’s second Action Plan on Open Government, the Government of Canada has committed to the disclosure of contracting data via a centralized, machine-readable database available to the public. This commitment reinforces the proactive publication of contracts, which reflects broader government commitments to transparency and strengthened accountability within the public sector originally announced in Budget 2004.\n",
      "--------------------------\n",
      "Prediction (Post fine tuning):  Dans le cadre du deuxième plan d'action du Canada sur l'État ouvert, le gouvernement du Canada s'est engagé à communiquer des données sous-traitées grâce à une base de données centralisée et lisible par les machines, à la disposition du public. Cet engagement renforce la publication proactive des marchés, qui reflète l'engagement plus large du gouvernement à la transparence et au renforcement de la redevabilité dans le secteur public, qui avait été initialement annoncé dans le budget de 2004.\n",
      "--------------------------\n",
      "Reference:  Dans le cadre du deuxième Plan d’action national pour un gouvernement ouvert du Canada, le gouvernement du Canada s’est engagé à la divulgation des données sur la passation de marchés au moyen d’une base de données publique à la fois centralisée et lisible par machine. Cet engagement renforce la publication proactive des marchés, qui tient compte des engagements élargis du gouvernement envers la transparence et la responsabilisation accrue dans le secteur public, annoncés initialement dans le budget fédéral de 2004.\n",
      "--------------------------\n",
      "Original Prediction (Pre fine tuning):  Dans le cadre du deuxième Plan d'action du Canada pour un gouvernement ouvert, le gouvernement du Canada s'est engagé à communiquer des données sous-traitées via une base de données centralisée, lisible par la machine, accessible au public. Cet objectif renforce la publication proactive des marchés, qui reflète l'ensemble des promesses gouvernementales en matière de transparence et de redevabilité au sein du secteur public initialement annoncé dans le budget de 2004.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "input_text = \"As part of Canada’s second Action Plan on Open Government, the Government of Canada has committed to the disclosure of contracting data via a centralized, machine-readable database available to the public. This commitment reinforces the proactive publication of contracts, which reflects broader government commitments to transparency and strengthened accountability within the public sector originally announced in Budget 2004.\"\n",
    "reference_text = \"Dans le cadre du deuxième Plan d’action national pour un gouvernement ouvert du Canada, le gouvernement du Canada s’est engagé à la divulgation des données sur la passation de marchés au moyen d’une base de données publique à la fois centralisée et lisible par machine. Cet engagement renforce la publication proactive des marchés, qui tient compte des engagements élargis du gouvernement envers la transparence et la responsabilisation accrue dans le secteur public, annoncés initialement dans le budget fédéral de 2004.\"\n",
    "original_prediction = \"Dans le cadre du deuxième Plan d'action du Canada pour un gouvernement ouvert, le gouvernement du Canada s'est engagé à communiquer des données sous-traitées via une base de données centralisée, lisible par la machine, accessible au public. Cet objectif renforce la publication proactive des marchés, qui reflète l'ensemble des promesses gouvernementales en matière de transparence et de redevabilité au sein du secteur public initialement annoncé dans le budget de 2004.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}  # Move inputs to the same device as the model\n",
    "translated = model.generate(**inputs)\n",
    "print(\"Input: \", input_text)\n",
    "#print(textwrap.fill(input_text, width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Prediction (Post fine tuning): \", tokenizer.decode(translated[0], skip_special_tokens=True))\n",
    "#print(textwrap.fill(tokenizer.decode(translated[0], skip_special_tokens=True), width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Reference: \", reference_text)\n",
    "#print(textwrap.fill(reference_text, width=120))\n",
    "print(\"--------------------------\")\n",
    "print(\"Original Prediction (Pre fine tuning): \", original_prediction)\n",
    "#print(textwrap.fill(original_prediction, width=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
