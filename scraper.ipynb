{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63e38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from lxml import etree as ET  # Replace the existing xml.etree.ElementTree import\n",
    "import re\n",
    "import unicodedata\n",
    "import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16e836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted document IDs:\n",
      "[12084, 12111, 12129, 12139, 12141, 12143, 12160, 12182, 12323, 12453, 12510, 12522, 12553, 12563, 12583, 12588, 12595, 12601, 12602, 12607, 12610, 12614, 13342, 13525, 13583, 13589, 13593, 13602, 13603, 13616, 13663, 13685, 13697, 13832, 13848, 13890, 13937, 13953, 13954, 14208, 14219, 14265, 15772, 15773, 15774, 15796, 16484, 16553, 16557, 16577, 16578, 17065, 17067, 17151, 17280, 17284, 17590, 18309, 18310, 19061, 19420, 19421, 19422, 20008, 20930, 21104, 22370, 22379, 23601, 24227, 24970, 25049, 25583, 25593, 25600, 25748, 25761, 25845, 25857, 25867, 25868, 25875, 26160, 26163, 26164, 26168, 26262, 26295, 26332, 26952, 26953, 26954, 27088, 27146, 27228, 27256, 27807, 28108, 28203, 28305, 28699, 28700, 30656, 30678, 30682, 30683, 31300, 31306, 32495, 32499, 32502, 32503, 32504, 32505, 32509, 32510, 32511, 32512, 32513, 32514, 32515, 32516, 32517, 32518, 32519, 32520, 32521, 32522, 32523, 32524, 32525, 32526, 32527, 32528, 32529, 32530, 32533, 32563, 32573, 32581, 32582, 32583, 32590, 32592, 32593, 32594, 32600, 32601, 32603, 32610, 32611, 32612, 32613, 32614, 32616, 32620, 32621, 32625, 32628, 32634, 32636, 32646, 32647, 32648, 32649, 32650, 32660, 32662, 32663, 32664, 32665, 32671, 32673, 32687, 32688, 32689, 32690, 32692, 32709, 32710, 32711, 32712, 32713, 32714, 32715, 32716, 32728, 32736, 32743, 32744, 32749, 32756, 32775, 32780, 32784, 32785, 32786, 32787, 32802, 32805, 32808, 32810, 32814]\n"
     ]
    }
   ],
   "source": [
    "# Get IDs\n",
    "# Get IDs\n",
    "# URL of the policy instruments page\n",
    "url = 'https://www.tbs-sct.canada.ca/pol/a-z-eng.aspx'\n",
    "\n",
    "# Set headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all anchor tags with the document links and extract IDs\n",
    "doc_ids_set = set()\n",
    "for tag in soup.find_all('a', href=True):\n",
    "    match = re.search(r'doc-eng\\.aspx\\?id=(\\d+)', tag['href'])\n",
    "    if match:\n",
    "        doc_ids_set.add(int(match.group(1)))\n",
    "\n",
    "print(\"Extracted document IDs:\")\n",
    "doc_ids = sorted(doc_ids_set)\n",
    "print(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463bf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ids = [\"12182\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0155a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_ai(text: str) -> str:\n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    # Unescape HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove non-printable/control characters\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Remove leading/trailing whitespace again\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def fetch_xml(doc_id: int, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the XML section for the given document ID and language code.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.tbs-sct.canada.ca/pol/doc-{lang}.aspx?id={doc_id}&section=xml\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddd8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes_xml(xml_bytes: bytes) -> list[str]:\n",
    "    \"\"\"\n",
    "    Parses XML bytes, finds all <p> and <li> within the main body,\n",
    "    excluding content in appendices sections.\n",
    "    Returns their text contents as a list.\n",
    "    \"\"\"\n",
    "    parser = ET.XMLParser(encoding='utf-8')\n",
    "    root = ET.fromstring(xml_bytes, parser=parser)\n",
    "    items = []\n",
    "    \n",
    "    # Find all appendices sections first\n",
    "    appendices = root.findall(\".//appendices\")\n",
    "    \n",
    "    def is_in_appendices(elem):\n",
    "        \"\"\"Check if element is inside any appendices section\"\"\"\n",
    "        parent = elem.getparent()\n",
    "        while parent is not None:\n",
    "            if parent in appendices:\n",
    "                return True\n",
    "            parent = parent.getparent()\n",
    "        return False\n",
    "\n",
    "    # Find all p and li elements\n",
    "    for elem in root.findall(\".//p\") + root.findall(\".//li\") + root.findall(\".//clause\"):\n",
    "        if is_in_appendices(elem):\n",
    "            continue\n",
    "            \n",
    "        # Get all text content, including tail text\n",
    "        text_parts = []\n",
    "        if elem.text:\n",
    "            text_parts.append(elem.text)\n",
    "        for child in elem:\n",
    "            if child.tail:\n",
    "                text_parts.append(child.tail)\n",
    "        \n",
    "        text = \" \".join(text_parts)\n",
    "        clean = clean_text_for_ai(text)\n",
    "        if len(clean) > 2:  # skip empty/very short\n",
    "            items.append(clean)\n",
    "            \n",
    "    return items\n",
    "\n",
    "def scrape_doc_bilingual(doc_id: int) -> dict[str, list[str]]:\n",
    "    data = {}\n",
    "    for lang_code, key in [(\"eng\", \"en\"), (\"fra\", \"fr\")]:\n",
    "        xml = fetch_xml(doc_id, lang_code)\n",
    "        items = extract_nodes_xml(xml)\n",
    "        data[key] = items\n",
    "    # Align by index, truncate to shortest length\n",
    "    min_len = min(len(data[\"en\"]), len(data[\"fr\"]))\n",
    "    pairs = [{\"en\": data[\"en\"][i], \"fr\": data[\"fr\"][i]} for i in range(min_len)]\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fdb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_block_children_with_anchor(xml_bytes: bytes) -> list[dict]:\n",
    "    \"\"\"\n",
    "    For each element with an anchor, extract each direct child block-level element as its own entry,\n",
    "    using the parent's anchor. Skips elements with class containing 'hidden' or 'invisible'.\n",
    "    Returns a list of {\"anchor\": anchor, \"tag\": tag, \"text\": text}.\n",
    "    \"\"\"\n",
    "    BLOCK_TAGS = {\n",
    "        \"section\", \"div\", \"p\", \"li\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\",\n",
    "        \"chapter\", \"clause\", \"article\", \"ul\", \"ol\", \"table\", \"thead\", \"tbody\", \"tr\", \"td\", \"th\"\n",
    "    }\n",
    "    parser = ET.XMLParser(encoding='utf-8')\n",
    "    root = ET.fromstring(xml_bytes, parser=parser)\n",
    "    items = []\n",
    "\n",
    "    appendices = root.findall(\".//appendices\")\n",
    "\n",
    "    def is_in_appendices(elem):\n",
    "        parent = elem.getparent()\n",
    "        while parent is not None:\n",
    "            if parent in appendices:\n",
    "                return True\n",
    "            parent = parent.getparent()\n",
    "        return False\n",
    "\n",
    "    def has_hidden_class(elem):\n",
    "        cls = elem.get(\"class\", \"\")\n",
    "        return any(word in cls for word in (\"hidden\", \"invisible\"))\n",
    "\n",
    "    for elem in root.iter():\n",
    "        anchor = elem.get(\"anchor\")\n",
    "        if anchor:\n",
    "            for child in elem:\n",
    "                if child.tag in BLOCK_TAGS and not is_in_appendices(child):\n",
    "                    # Skip if this element or any descendant has a hidden/invisible class\n",
    "                    skip = False\n",
    "                    for descendant in child.iter():\n",
    "                        if has_hidden_class(descendant):\n",
    "                            skip = True\n",
    "                            break\n",
    "                    if skip:\n",
    "                        continue\n",
    "                    text = \"\".join(child.itertext()).strip()\n",
    "                    clean = clean_text_for_ai(text)\n",
    "                    if clean and len(clean) > 2:\n",
    "                        items.append({\n",
    "                            \"anchor\": anchor,\n",
    "                            \"tag\": child.tag,\n",
    "                            \"text\": clean\n",
    "                        })\n",
    "    return items\n",
    "\n",
    "def scrape_doc_bilingual_by_anchor(doc_id: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Aligns English and French block-level nodes by anchor property and order.\n",
    "    Returns a list of {\"en\": ..., \"fr\": ..., \"anchor\": ..., \"tag\": ...}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for lang_code, key in [(\"eng\", \"en\"), (\"fra\", \"fr\")]:\n",
    "        xml = fetch_xml(doc_id, lang_code)\n",
    "        items = extract_block_children_with_anchor(xml)\n",
    "        data[key] = items\n",
    "\n",
    "    # Build anchor -> list of items mapping for each language\n",
    "    def build_map(items):\n",
    "        amap = {}\n",
    "        for item in items:\n",
    "            if item[\"anchor\"]:\n",
    "                amap.setdefault(item[\"anchor\"], []).append(item)\n",
    "        return amap\n",
    "\n",
    "    en_map = build_map(data[\"en\"])\n",
    "    fr_map = build_map(data[\"fr\"])\n",
    "\n",
    "    # Only keep anchors present in both\n",
    "    common_anchors = set(en_map) & set(fr_map)\n",
    "    pairs = []\n",
    "    for anchor in sorted(common_anchors):\n",
    "        en_items = en_map[anchor]\n",
    "        fr_items = fr_map[anchor]\n",
    "        min_len = min(len(en_items), len(fr_items))\n",
    "        for i in range(min_len):\n",
    "            pairs.append({\n",
    "                \"en\": en_items[i][\"text\"],\n",
    "                \"fr\": fr_items[i][\"text\"],\n",
    "                \"anchor\": anchor,\n",
    "                \"tag\": en_items[i][\"tag\"]\n",
    "            })\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3007bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new scraping run...\n",
      "\n",
      "Skipping doc_id 12084 (output exists)\n",
      "Skipping doc_id 12111 (output exists)\n",
      "Skipping doc_id 12129 (output exists)\n",
      "Skipping doc_id 12139 (output exists)\n",
      "Skipping doc_id 12141 (output exists)\n",
      "Skipping doc_id 12143 (output exists)\n",
      "Skipping doc_id 12160 (output exists)\n",
      "Skipping doc_id 12182 (output exists)\n",
      "Skipping doc_id 12323 (output exists)\n",
      "Skipping doc_id 12453 (output exists)\n",
      "Skipping doc_id 12510 (output exists)\n",
      "Skipping doc_id 12522 (output exists)\n",
      "Skipping doc_id 12553 (output exists)\n",
      "Skipping doc_id 12563 (output exists)\n",
      "Skipping doc_id 12583 (output exists)\n",
      "Skipping doc_id 12588 (output exists)\n",
      "Skipping doc_id 12595 (output exists)\n",
      "Skipping doc_id 12601 (output exists)\n",
      "Skipping doc_id 12602 (output exists)\n",
      "Skipping doc_id 12607 (output exists)\n",
      "Skipping doc_id 12610 (output exists)\n",
      "Skipping doc_id 12614 (output exists)\n",
      "Skipping doc_id 13342 (output exists)\n",
      "Skipping doc_id 13525 (output exists)\n",
      "Skipping doc_id 13583 (output exists)\n",
      "Skipping doc_id 13589 (output exists)\n",
      "Skipping doc_id 13593 (output exists)\n",
      "Skipping doc_id 13602 (output exists)\n",
      "Skipping doc_id 13603 (output exists)\n",
      "Skipping doc_id 13616 (output exists)\n",
      "Skipping doc_id 13663 (output exists)\n",
      "Skipping doc_id 13685 (output exists)\n",
      "Skipping doc_id 13697 (output exists)\n",
      "Skipping doc_id 13832 (output exists)\n",
      "Skipping doc_id 13848 (output exists)\n",
      "Skipping doc_id 13890 (output exists)\n",
      "Skipping doc_id 13937 (output exists)\n",
      "Skipping doc_id 13953 (output exists)\n",
      "Skipping doc_id 13954 (output exists)\n",
      "Skipping doc_id 14208 (output exists)\n",
      "Skipping doc_id 14219 (output exists)\n",
      "Skipping doc_id 14265 (output exists)\n",
      "Skipping doc_id 15772 (output exists)\n",
      "Skipping doc_id 15773 (output exists)\n",
      "Skipping doc_id 15774 (output exists)\n",
      "Skipping doc_id 15796 (output exists)\n",
      "Skipping doc_id 16484 (output exists)\n",
      "Skipping doc_id 16553 (output exists)\n",
      "Skipping doc_id 16557 (output exists)\n",
      "Skipping doc_id 16577 (output exists)\n",
      "Skipping doc_id 16578 (output exists)\n",
      "Skipping doc_id 17065 (output exists)\n",
      "Skipping doc_id 17067 (output exists)\n",
      "Skipping doc_id 17151 (output exists)\n",
      "Skipping doc_id 17280 (output exists)\n",
      "Skipping doc_id 17284 (output exists)\n",
      "Skipping doc_id 17590 (output exists)\n",
      "Skipping doc_id 18309 (output exists)\n",
      "Skipping doc_id 18310 (output exists)\n",
      "Skipping doc_id 19061 (output exists)\n",
      "Skipping doc_id 19420 (output exists)\n",
      "Skipping doc_id 19421 (output exists)\n",
      "Skipping doc_id 19422 (output exists)\n",
      "Skipping doc_id 20008 (output exists)\n",
      "Skipping doc_id 20930 (output exists)\n",
      "Skipping doc_id 21104 (output exists)\n",
      "Skipping doc_id 22370 (output exists)\n",
      "Skipping doc_id 22379 (output exists)\n",
      "Skipping doc_id 23601 (output exists)\n",
      "Skipping doc_id 24227 (output exists)\n",
      "Skipping doc_id 24970 (output exists)\n",
      "Skipping doc_id 25049 (output exists)\n",
      "Skipping doc_id 25583 (output exists)\n",
      "Skipping doc_id 25593 (output exists)\n",
      "Skipping doc_id 25600 (output exists)\n",
      "Skipping doc_id 25748 (output exists)\n",
      "Skipping doc_id 25761 (output exists)\n",
      "Skipping doc_id 25845 (output exists)\n",
      "Skipping doc_id 25857 (output exists)\n",
      "Skipping doc_id 25867 (output exists)\n",
      "Skipping doc_id 25868 (output exists)\n",
      "Skipping doc_id 25875 (output exists)\n",
      "Skipping doc_id 26160 (output exists)\n",
      "Skipping doc_id 26163 (output exists)\n",
      "Skipping doc_id 26164 (output exists)\n",
      "Skipping doc_id 26168 (output exists)\n",
      "Skipping doc_id 26262 (output exists)\n",
      "Skipping doc_id 26295 (output exists)\n",
      "Skipping doc_id 26332 (output exists)\n",
      "Skipping doc_id 26952 (output exists)\n",
      "Skipping doc_id 26953 (output exists)\n",
      "Skipping doc_id 26954 (output exists)\n",
      "Skipping doc_id 27088 (output exists)\n",
      "Skipping doc_id 27146 (output exists)\n",
      "Skipping doc_id 27228 (output exists)\n",
      "Skipping doc_id 27256 (output exists)\n",
      "Skipping doc_id 27807 (output exists)\n",
      "Skipping doc_id 28108 (output exists)\n",
      "Skipping doc_id 28203 (output exists)\n",
      "Skipping doc_id 28305 (output exists)\n",
      "Skipping doc_id 28699 (output exists)\n",
      "Skipping doc_id 28700 (output exists)\n",
      "Skipping doc_id 30656 (output exists)\n",
      "Skipping doc_id 30678 (output exists)\n",
      "Skipping doc_id 30682 (output exists)\n",
      "Skipping doc_id 30683 (output exists)\n",
      "Skipping doc_id 31300 (output exists)\n",
      "Skipping doc_id 31306 (output exists)\n",
      "Skipping doc_id 32495 (output exists)\n",
      "Skipping doc_id 32499 (output exists)\n",
      "Skipping doc_id 32502 (output exists)\n",
      "Skipping doc_id 32503 (output exists)\n",
      "Error on ID 32504: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32505 (output exists)\n",
      "Skipping doc_id 32509 (output exists)\n",
      "Skipping doc_id 32510 (output exists)\n",
      "Skipping doc_id 32511 (output exists)\n",
      "Skipping doc_id 32512 (output exists)\n",
      "Skipping doc_id 32513 (output exists)\n",
      "Skipping doc_id 32514 (output exists)\n",
      "Error on ID 32515: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32516 (output exists)\n",
      "Skipping doc_id 32517 (output exists)\n",
      "Skipping doc_id 32518 (output exists)\n",
      "Error on ID 32519: argument of type 'NoneType' is not iterable\n",
      "Error on ID 32520: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32521 (output exists)\n",
      "Skipping doc_id 32522 (output exists)\n",
      "Skipping doc_id 32523 (output exists)\n",
      "Error on ID 32524: argument of type 'NoneType' is not iterable\n",
      "Error on ID 32525: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32526 (output exists)\n",
      "Skipping doc_id 32527 (output exists)\n",
      "Error on ID 32528: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32529 (output exists)\n",
      "Skipping doc_id 32530 (output exists)\n",
      "Skipping doc_id 32533 (output exists)\n",
      "Skipping doc_id 32563 (output exists)\n",
      "Skipping doc_id 32573 (output exists)\n",
      "Skipping doc_id 32581 (output exists)\n",
      "Skipping doc_id 32582 (output exists)\n",
      "Skipping doc_id 32583 (output exists)\n",
      "Skipping doc_id 32590 (output exists)\n",
      "Skipping doc_id 32592 (output exists)\n",
      "Skipping doc_id 32593 (output exists)\n",
      "Skipping doc_id 32594 (output exists)\n",
      "Skipping doc_id 32600 (output exists)\n",
      "Skipping doc_id 32601 (output exists)\n",
      "Skipping doc_id 32603 (output exists)\n",
      "Skipping doc_id 32610 (output exists)\n",
      "Skipping doc_id 32611 (output exists)\n",
      "Skipping doc_id 32612 (output exists)\n",
      "Skipping doc_id 32613 (output exists)\n",
      "Skipping doc_id 32614 (output exists)\n",
      "Skipping doc_id 32616 (output exists)\n",
      "Skipping doc_id 32620 (output exists)\n",
      "Skipping doc_id 32621 (output exists)\n",
      "Skipping doc_id 32625 (output exists)\n",
      "Skipping doc_id 32628 (output exists)\n",
      "Skipping doc_id 32634 (output exists)\n",
      "Skipping doc_id 32636 (output exists)\n",
      "Skipping doc_id 32646 (output exists)\n",
      "Skipping doc_id 32647 (output exists)\n",
      "Skipping doc_id 32648 (output exists)\n",
      "Skipping doc_id 32649 (output exists)\n",
      "Skipping doc_id 32650 (output exists)\n",
      "Skipping doc_id 32660 (output exists)\n",
      "Skipping doc_id 32662 (output exists)\n",
      "Skipping doc_id 32663 (output exists)\n",
      "Skipping doc_id 32664 (output exists)\n",
      "Skipping doc_id 32665 (output exists)\n",
      "Skipping doc_id 32671 (output exists)\n",
      "Skipping doc_id 32673 (output exists)\n",
      "Skipping doc_id 32687 (output exists)\n",
      "Skipping doc_id 32688 (output exists)\n",
      "Skipping doc_id 32689 (output exists)\n",
      "Skipping doc_id 32690 (output exists)\n",
      "Skipping doc_id 32692 (output exists)\n",
      "Skipping doc_id 32709 (output exists)\n",
      "Skipping doc_id 32710 (output exists)\n",
      "Skipping doc_id 32711 (output exists)\n",
      "Skipping doc_id 32712 (output exists)\n",
      "Skipping doc_id 32713 (output exists)\n",
      "Skipping doc_id 32714 (output exists)\n",
      "Skipping doc_id 32715 (output exists)\n",
      "Skipping doc_id 32716 (output exists)\n",
      "Error on ID 32728: argument of type 'NoneType' is not iterable\n",
      "Error on ID 32736: argument of type 'NoneType' is not iterable\n",
      "Skipping doc_id 32743 (output exists)\n",
      "Skipping doc_id 32744 (output exists)\n",
      "Skipping doc_id 32749 (output exists)\n",
      "Skipping doc_id 32756 (output exists)\n",
      "Skipping doc_id 32775 (output exists)\n",
      "Skipping doc_id 32780 (output exists)\n",
      "Skipping doc_id 32784 (output exists)\n",
      "Skipping doc_id 32785 (output exists)\n",
      "Skipping doc_id 32786 (output exists)\n",
      "Skipping doc_id 32787 (output exists)\n",
      "Skipping doc_id 32802 (output exists)\n",
      "Skipping doc_id 32805 (output exists)\n",
      "Skipping doc_id 32808 (output exists)\n",
      "Skipping doc_id 32810 (output exists)\n",
      "Skipping doc_id 32814 (output exists)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting new scraping run...\\n\")\n",
    "all_docs = {}\n",
    "\n",
    "# Ensure output directory exists\n",
    "out_path = Path(\"output/docs\")\n",
    "out_path.mkdir(exist_ok=True)\n",
    "\n",
    "for did in doc_ids:\n",
    "    out_file = out_path / f\"{did}.json\"\n",
    "    if out_file.exists():\n",
    "        print(f\"Skipping doc_id {did} (output exists)\")\n",
    "        continue\n",
    "    try:\n",
    "        all_docs[str(did)] = scrape_doc_bilingual_by_anchor(did)\n",
    "        print(\"doc_id: \", did,  \" - Pairs: \", len(all_docs[str(did)]))\n",
    "        with open(out_path / f\"{did}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_docs[str(did)], f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on ID {did}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d2e19f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8128 pairs to output\\all_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# Collect all en/fr pairs from each JSON file in output/docs\n",
    "all_pairs = []\n",
    "all_pairs_indexed = []\n",
    "docs_path = Path(\"output/docs\")\n",
    "\n",
    "\n",
    "for file in docs_path.glob(\"*.json\"):\n",
    "    did = file.stem\n",
    "    if did in skip_ids:\n",
    "        continue\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_pairs = json.load(f)\n",
    "        # If the file contains a dict (old format), flatten its values\n",
    "        if isinstance(doc_pairs, dict):\n",
    "            for pairs in doc_pairs.values():\n",
    "                for pair in pairs:\n",
    "                    all_pairs.append({\"en\": pair.get(\"en\", \"\"), \"fr\": pair.get(\"fr\", \"\")})\n",
    "                    indexed = dict(pair)\n",
    "                    indexed[\"did\"] = did\n",
    "                    all_pairs_indexed.append(indexed)\n",
    "        else:\n",
    "            for pair in doc_pairs:\n",
    "                all_pairs.append({\"en\": pair.get(\"en\", \"\"), \"fr\": pair.get(\"fr\", \"\")})\n",
    "                indexed = dict(pair)\n",
    "                indexed[\"did\"] = did\n",
    "                all_pairs_indexed.append(indexed)\n",
    "\n",
    "# Write the flattened list to a new JSON file\n",
    "with open(Path(\"output\") / \"all_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Write the indexed list to a new JSON file\n",
    "with open(Path(\"output\") / \"all_pairs_indexed.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_pairs_indexed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(all_pairs)} pairs to {Path('output') / 'all_pairs.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
