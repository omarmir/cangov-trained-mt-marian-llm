{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63e38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from lxml import etree as ET  # Replace the existing xml.etree.ElementTree import\n",
    "import re\n",
    "import unicodedata\n",
    "import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16e836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted document IDs:\n",
      "[12084, 12111, 12129, 12139, 12141, 12143, 12160, 12182, 12323, 12453, 12510, 12522, 12553, 12563, 12583, 12588, 12595, 12601, 12602, 12607, 12610, 12614, 13342, 13525, 13583, 13589, 13593, 13602, 13603, 13616, 13663, 13685, 13697, 13832, 13848, 13890, 13937, 13953, 13954, 14208, 14219, 14265, 15772, 15773, 15774, 15796, 16484, 16553, 16557, 16577, 16578, 17065, 17067, 17151, 17280, 17284, 17590, 18309, 18310, 19061, 19420, 19421, 19422, 20008, 20930, 21104, 22370, 22379, 23601, 24227, 24970, 25049, 25583, 25593, 25600, 25748, 25761, 25845, 25857, 25867, 25868, 25875, 26160, 26163, 26164, 26168, 26262, 26295, 26332, 26952, 26953, 26954, 27088, 27146, 27228, 27256, 27807, 28108, 28203, 28305, 28699, 28700, 30656, 30678, 30682, 30683, 31300, 31306, 32495, 32499, 32502, 32503, 32504, 32505, 32509, 32510, 32511, 32512, 32513, 32514, 32515, 32516, 32517, 32518, 32519, 32520, 32521, 32522, 32523, 32524, 32525, 32526, 32527, 32528, 32529, 32530, 32533, 32563, 32573, 32581, 32582, 32583, 32590, 32592, 32593, 32594, 32600, 32601, 32603, 32610, 32611, 32612, 32613, 32614, 32616, 32620, 32621, 32625, 32628, 32634, 32636, 32646, 32647, 32648, 32649, 32650, 32660, 32662, 32663, 32664, 32665, 32671, 32673, 32687, 32688, 32689, 32690, 32692, 32709, 32710, 32711, 32712, 32713, 32714, 32715, 32716, 32728, 32736, 32743, 32744, 32749, 32756, 32775, 32780, 32784, 32785, 32786, 32787, 32802, 32805, 32808, 32810, 32814]\n"
     ]
    }
   ],
   "source": [
    "# Get IDs\n",
    "# Get IDs\n",
    "# URL of the policy instruments page\n",
    "url = 'https://www.tbs-sct.canada.ca/pol/a-z-eng.aspx'\n",
    "\n",
    "# Set headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all anchor tags with the document links and extract IDs\n",
    "doc_ids_set = set()\n",
    "for tag in soup.find_all('a', href=True):\n",
    "    match = re.search(r'doc-eng\\.aspx\\?id=(\\d+)', tag['href'])\n",
    "    if match:\n",
    "        doc_ids_set.add(int(match.group(1)))\n",
    "\n",
    "print(\"Extracted document IDs:\")\n",
    "doc_ids = sorted(doc_ids_set)\n",
    "print(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463bf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_ids = [32763, 32692, 13525, 32616, 19421]  # extend as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddd8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_ai(text: str) -> str:\n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    # Unescape HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove non-printable/control characters\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Remove leading/trailing whitespace again\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def fetch_xml(doc_id: int, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the XML section for the given document ID and language code.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.tbs-sct.canada.ca/pol/doc-{lang}.aspx?id={doc_id}&section=xml\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.content\n",
    "\n",
    "def extract_nodes_xml(xml_bytes: bytes) -> list[str]:\n",
    "    \"\"\"\n",
    "    Parses XML bytes, finds all <p> and <li> within the main body,\n",
    "    excluding content in appendices sections.\n",
    "    Returns their text contents as a list.\n",
    "    \"\"\"\n",
    "    parser = ET.XMLParser(encoding='utf-8')\n",
    "    root = ET.fromstring(xml_bytes, parser=parser)\n",
    "    items = []\n",
    "    \n",
    "    # Find all appendices sections first\n",
    "    appendices = root.findall(\".//appendices\")\n",
    "    \n",
    "    def is_in_appendices(elem):\n",
    "        \"\"\"Check if element is inside any appendices section\"\"\"\n",
    "        parent = elem.getparent()\n",
    "        while parent is not None:\n",
    "            if parent in appendices:\n",
    "                return True\n",
    "            parent = parent.getparent()\n",
    "        return False\n",
    "\n",
    "    # Find all p and li elements\n",
    "    for elem in root.findall(\".//p\") + root.findall(\".//li\") + root.findall(\".//clause\"):\n",
    "        if is_in_appendices(elem):\n",
    "            continue\n",
    "            \n",
    "        # Get all text content, including tail text\n",
    "        text_parts = []\n",
    "        if elem.text:\n",
    "            text_parts.append(elem.text)\n",
    "        for child in elem:\n",
    "            if child.tail:\n",
    "                text_parts.append(child.tail)\n",
    "        \n",
    "        text = \" \".join(text_parts)\n",
    "        clean = clean_text_for_ai(text)\n",
    "        if len(clean) > 2:  # skip empty/very short\n",
    "            items.append(clean)\n",
    "            \n",
    "    return items\n",
    "\n",
    "def scrape_doc_bilingual(doc_id: int) -> dict[str, list[str]]:\n",
    "    data = {}\n",
    "    for lang_code, key in [(\"eng\", \"en\"), (\"fra\", \"fr\")]:\n",
    "        xml = fetch_xml(doc_id, lang_code)\n",
    "        items = extract_nodes_xml(xml)\n",
    "        data[key] = items\n",
    "    # Align by index, truncate to shortest length\n",
    "    min_len = min(len(data[\"en\"]), len(data[\"fr\"]))\n",
    "    pairs = [{\"en\": data[\"en\"][i], \"fr\": data[\"fr\"][i]} for i in range(min_len)]\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3007bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new scraping run...\n",
      "\n",
      "doc_id:  12084  - Pairs:  31\n",
      "doc_id:  12111  - Pairs:  37\n",
      "doc_id:  12129  - Pairs:  14\n",
      "doc_id:  12139  - Pairs:  14\n",
      "doc_id:  12141  - Pairs:  10\n",
      "doc_id:  12143  - Pairs:  26\n",
      "doc_id:  12160  - Pairs:  23\n",
      "doc_id:  12182  - Pairs:  4654\n",
      "doc_id:  12323  - Pairs:  135\n",
      "doc_id:  12453  - Pairs:  106\n",
      "doc_id:  12510  - Pairs:  111\n",
      "doc_id:  12522  - Pairs:  2\n",
      "doc_id:  12553  - Pairs:  71\n",
      "doc_id:  12563  - Pairs:  11\n",
      "doc_id:  12583  - Pairs:  26\n",
      "doc_id:  12588  - Pairs:  16\n",
      "doc_id:  12595  - Pairs:  62\n",
      "doc_id:  12601  - Pairs:  50\n",
      "doc_id:  12602  - Pairs:  55\n",
      "doc_id:  12607  - Pairs:  174\n",
      "doc_id:  12610  - Pairs:  18\n",
      "doc_id:  12614  - Pairs:  45\n",
      "doc_id:  13342  - Pairs:  49\n",
      "doc_id:  13525  - Pairs:  88\n",
      "doc_id:  13583  - Pairs:  22\n",
      "doc_id:  13589  - Pairs:  39\n",
      "doc_id:  13593  - Pairs:  2\n",
      "doc_id:  13602  - Pairs:  283\n",
      "doc_id:  13603  - Pairs:  84\n",
      "doc_id:  13616  - Pairs:  76\n",
      "doc_id:  13663  - Pairs:  2\n",
      "doc_id:  13685  - Pairs:  8\n",
      "doc_id:  13697  - Pairs:  54\n",
      "doc_id:  13832  - Pairs:  277\n",
      "doc_id:  13848  - Pairs:  58\n",
      "doc_id:  13890  - Pairs:  25\n",
      "doc_id:  13937  - Pairs:  89\n",
      "doc_id:  13953  - Pairs:  13\n",
      "doc_id:  13954  - Pairs:  16\n",
      "doc_id:  14208  - Pairs:  83\n",
      "doc_id:  14219  - Pairs:  37\n",
      "doc_id:  14265  - Pairs:  27\n",
      "doc_id:  15772  - Pairs:  55\n",
      "doc_id:  15773  - Pairs:  17\n",
      "doc_id:  15774  - Pairs:  55\n",
      "doc_id:  15796  - Pairs:  79\n",
      "doc_id:  16484  - Pairs:  110\n",
      "doc_id:  16553  - Pairs:  48\n",
      "doc_id:  16557  - Pairs:  195\n",
      "doc_id:  16577  - Pairs:  36\n",
      "doc_id:  16578  - Pairs:  127\n",
      "doc_id:  17065  - Pairs:  222\n",
      "doc_id:  17067  - Pairs:  41\n",
      "doc_id:  17151  - Pairs:  79\n",
      "doc_id:  17280  - Pairs:  46\n",
      "doc_id:  17284  - Pairs:  52\n",
      "doc_id:  17590  - Pairs:  43\n",
      "doc_id:  18309  - Pairs:  160\n",
      "doc_id:  18310  - Pairs:  107\n",
      "doc_id:  19061  - Pairs:  158\n",
      "doc_id:  19420  - Pairs:  52\n",
      "doc_id:  19421  - Pairs:  166\n",
      "doc_id:  19422  - Pairs:  39\n",
      "doc_id:  20008  - Pairs:  107\n",
      "doc_id:  20930  - Pairs:  42\n",
      "doc_id:  21104  - Pairs:  29\n",
      "doc_id:  22370  - Pairs:  18\n",
      "doc_id:  22379  - Pairs:  48\n",
      "doc_id:  23601  - Pairs:  68\n",
      "doc_id:  24227  - Pairs:  58\n",
      "doc_id:  24970  - Pairs:  691\n",
      "doc_id:  25049  - Pairs:  37\n",
      "doc_id:  25583  - Pairs:  107\n",
      "doc_id:  25593  - Pairs:  31\n",
      "doc_id:  25600  - Pairs:  41\n",
      "doc_id:  25748  - Pairs:  149\n",
      "doc_id:  25761  - Pairs:  364\n",
      "doc_id:  25845  - Pairs:  30\n",
      "doc_id:  25857  - Pairs:  29\n",
      "doc_id:  25867  - Pairs:  8\n",
      "doc_id:  25868  - Pairs:  118\n",
      "doc_id:  25875  - Pairs:  57\n",
      "doc_id:  26160  - Pairs:  58\n",
      "doc_id:  26163  - Pairs:  74\n",
      "doc_id:  26164  - Pairs:  83\n",
      "doc_id:  26168  - Pairs:  73\n",
      "doc_id:  26262  - Pairs:  254\n",
      "doc_id:  26295  - Pairs:  16\n",
      "doc_id:  26332  - Pairs:  556\n",
      "doc_id:  26952  - Pairs:  564\n",
      "doc_id:  26953  - Pairs:  675\n",
      "doc_id:  26954  - Pairs:  738\n",
      "doc_id:  27088  - Pairs:  101\n",
      "doc_id:  27146  - Pairs:  29\n",
      "doc_id:  27228  - Pairs:  24\n",
      "doc_id:  27256  - Pairs:  60\n",
      "doc_id:  27807  - Pairs:  135\n",
      "doc_id:  28108  - Pairs:  43\n",
      "doc_id:  28203  - Pairs:  1\n",
      "doc_id:  28305  - Pairs:  42\n",
      "doc_id:  28699  - Pairs:  25\n",
      "doc_id:  28700  - Pairs:  45\n",
      "doc_id:  30656  - Pairs:  57\n",
      "doc_id:  30678  - Pairs:  363\n",
      "doc_id:  30682  - Pairs:  181\n",
      "doc_id:  30683  - Pairs:  149\n",
      "doc_id:  31300  - Pairs:  139\n",
      "doc_id:  31306  - Pairs:  81\n",
      "doc_id:  32495  - Pairs:  171\n",
      "doc_id:  32499  - Pairs:  47\n",
      "doc_id:  32502  - Pairs:  162\n",
      "doc_id:  32503  - Pairs:  60\n",
      "doc_id:  32504  - Pairs:  86\n",
      "doc_id:  32505  - Pairs:  125\n",
      "doc_id:  32509  - Pairs:  22\n",
      "doc_id:  32510  - Pairs:  29\n",
      "doc_id:  32511  - Pairs:  6\n",
      "doc_id:  32512  - Pairs:  11\n",
      "doc_id:  32513  - Pairs:  5\n",
      "doc_id:  32514  - Pairs:  9\n",
      "doc_id:  32515  - Pairs:  5\n",
      "doc_id:  32516  - Pairs:  5\n",
      "doc_id:  32517  - Pairs:  15\n",
      "doc_id:  32518  - Pairs:  97\n",
      "doc_id:  32519  - Pairs:  12\n",
      "doc_id:  32520  - Pairs:  24\n",
      "doc_id:  32521  - Pairs:  11\n",
      "doc_id:  32522  - Pairs:  5\n",
      "doc_id:  32523  - Pairs:  10\n",
      "doc_id:  32524  - Pairs:  15\n",
      "doc_id:  32525  - Pairs:  26\n",
      "doc_id:  32526  - Pairs:  14\n",
      "doc_id:  32527  - Pairs:  14\n",
      "doc_id:  32528  - Pairs:  60\n",
      "doc_id:  32529  - Pairs:  7\n",
      "doc_id:  32530  - Pairs:  70\n",
      "doc_id:  32533  - Pairs:  29\n",
      "doc_id:  32563  - Pairs:  42\n",
      "doc_id:  32573  - Pairs:  52\n",
      "doc_id:  32581  - Pairs:  135\n",
      "doc_id:  32582  - Pairs:  133\n",
      "doc_id:  32583  - Pairs:  75\n",
      "doc_id:  32590  - Pairs:  78\n",
      "doc_id:  32592  - Pairs:  64\n",
      "doc_id:  32593  - Pairs:  152\n",
      "doc_id:  32594  - Pairs:  131\n",
      "doc_id:  32600  - Pairs:  243\n",
      "doc_id:  32601  - Pairs:  120\n",
      "doc_id:  32603  - Pairs:  188\n",
      "doc_id:  32610  - Pairs:  205\n",
      "doc_id:  32611  - Pairs:  78\n",
      "doc_id:  32612  - Pairs:  45\n",
      "doc_id:  32613  - Pairs:  20\n",
      "doc_id:  32614  - Pairs:  33\n",
      "doc_id:  32616  - Pairs:  104\n",
      "doc_id:  32620  - Pairs:  22\n",
      "doc_id:  32621  - Pairs:  194\n",
      "doc_id:  32625  - Pairs:  50\n",
      "doc_id:  32628  - Pairs:  45\n",
      "doc_id:  32634  - Pairs:  43\n",
      "doc_id:  32636  - Pairs:  39\n",
      "doc_id:  32646  - Pairs:  83\n",
      "doc_id:  32647  - Pairs:  105\n",
      "doc_id:  32648  - Pairs:  312\n",
      "doc_id:  32649  - Pairs:  137\n",
      "doc_id:  32650  - Pairs:  249\n",
      "doc_id:  32660  - Pairs:  272\n",
      "doc_id:  32662  - Pairs:  27\n",
      "doc_id:  32663  - Pairs:  31\n",
      "doc_id:  32664  - Pairs:  5\n",
      "doc_id:  32665  - Pairs:  17\n",
      "doc_id:  32671  - Pairs:  43\n",
      "doc_id:  32673  - Pairs:  138\n",
      "doc_id:  32687  - Pairs:  46\n",
      "doc_id:  32688  - Pairs:  42\n",
      "doc_id:  32689  - Pairs:  181\n",
      "doc_id:  32690  - Pairs:  126\n",
      "doc_id:  32692  - Pairs:  293\n",
      "doc_id:  32709  - Pairs:  12\n",
      "doc_id:  32710  - Pairs:  24\n",
      "doc_id:  32711  - Pairs:  59\n",
      "doc_id:  32712  - Pairs:  23\n",
      "doc_id:  32713  - Pairs:  3\n",
      "doc_id:  32714  - Pairs:  20\n",
      "doc_id:  32715  - Pairs:  12\n",
      "doc_id:  32716  - Pairs:  13\n",
      "doc_id:  32728  - Pairs:  45\n",
      "doc_id:  32736  - Pairs:  179\n",
      "doc_id:  32743  - Pairs:  24\n",
      "doc_id:  32744  - Pairs:  38\n",
      "doc_id:  32749  - Pairs:  78\n",
      "doc_id:  32756  - Pairs:  34\n",
      "doc_id:  32775  - Pairs:  123\n",
      "doc_id:  32780  - Pairs:  80\n",
      "doc_id:  32784  - Pairs:  35\n",
      "doc_id:  32785  - Pairs:  25\n",
      "doc_id:  32786  - Pairs:  10\n",
      "doc_id:  32787  - Pairs:  388\n",
      "doc_id:  32802  - Pairs:  19\n",
      "doc_id:  32805  - Pairs:  162\n",
      "doc_id:  32808  - Pairs:  62\n",
      "doc_id:  32810  - Pairs:  100\n",
      "doc_id:  32814  - Pairs:  37\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting new scraping run...\\n\")\n",
    "all_docs = {}\n",
    "\n",
    "# Ensure output directory exists\n",
    "out_path = Path(\"output/docs\")\n",
    "out_path.mkdir(exist_ok=True)\n",
    "\n",
    "for did in doc_ids:\n",
    "    out_file = out_path / f\"{did}.json\"\n",
    "    if out_file.exists():\n",
    "        print(f\"Skipping doc_id {did} (output exists)\")\n",
    "        continue\n",
    "    try:\n",
    "        all_docs[str(did)] = scrape_doc_bilingual(did)\n",
    "        print(\"doc_id: \", did,  \" - Pairs: \", len(all_docs[str(did)]))\n",
    "        with open(out_path / f\"{did}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_docs[str(did)], f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on ID {did}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2e19f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 22663 pairs to output\\all_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# Collect all en/fr pairs from each JSON file in output/docs\n",
    "all_pairs = []\n",
    "all_pairs_indexed = []\n",
    "docs_path = Path(\"output/docs\")\n",
    "\n",
    "def strip_leading_non_capital(text, lang=\"fr\"):\n",
    "    # For English: A-Z; For French: A-Z plus accented capitals\n",
    "    if lang == \"fr\":\n",
    "        pattern = r\"^([^A-ZÀÂÄÇÉÈÊËÎÏÔÖÙÛÜŸ]*)([A-ZÀÂÄÇÉÈÊËÎÏÔÖÙÛÜŸ].*)\"\n",
    "    else:\n",
    "        pattern = r\"^([^A-Z]*)([A-Z].*)\"\n",
    "    m = re.match(pattern, text)\n",
    "    if m:\n",
    "        return m.group(2)\n",
    "    return text\n",
    "\n",
    "\n",
    "for file in docs_path.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_pairs = json.load(f)\n",
    "        # If the file contains a dict (old format), flatten its values\n",
    "        if isinstance(doc_pairs, dict):\n",
    "            for pairs in doc_pairs.values():\n",
    "                for pair in pairs:\n",
    "                    all_pairs.append(pair)\n",
    "                    indexed = dict(pair)\n",
    "                    indexed[\"did\"] = did\n",
    "                    all_pairs_indexed.append(indexed)\n",
    "        else:\n",
    "            for pair in doc_pairs:\n",
    "                all_pairs.append(pair)\n",
    "                indexed = dict(pair)\n",
    "                indexed[\"did\"] = did\n",
    "                all_pairs_indexed.append(indexed)\n",
    "\n",
    "# Capitalize French if English starts with a capital letter\n",
    "for pair in all_pairs:\n",
    "    en = pair.get(\"en\", \"\")\n",
    "    fr = pair.get(\"fr\", \"\")\n",
    "    # Clean English: remove leading non-capital-letter chars\n",
    "    if en:\n",
    "        en_clean = strip_leading_non_capital(en, lang=\"en\")\n",
    "        pair[\"en\"] = en_clean\n",
    "    # Clean French: remove leading non-capital-letter chars\n",
    "    if fr:\n",
    "        fr_clean = strip_leading_non_capital(fr, lang=\"fr\")\n",
    "        pair[\"fr\"] = fr_clean\n",
    "    # Capitalize French if English starts with a capital letter\n",
    "    if en and en[0].isupper() and fr:\n",
    "        if fr and not fr[0].isupper():\n",
    "            pair[\"fr\"] = fr[0].upper() + fr[1:]\n",
    "\n",
    "# Write the flattened list to a new JSON file\n",
    "with open(Path(\"output\") / \"all_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Write the indexed list to a new JSON file\n",
    "with open(Path(\"output\") / \"all_pairs_indexed.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_pairs_indexed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(all_pairs)} pairs to {Path('output') / 'all_pairs.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
